## CNN的研究背景

1、图像需要处理的数据量太大，成本高，效率低。

> 图像由像素构成，每个像素包括RGB这三个参数来表示颜色信息，一张1000×1000的像素，需要处理的3百万个参数。
>
> CNN需要解决的第一个问题就是**将复杂问题简化**，把大量参数降维再做处理。

2、图像数字化的过程很难保留原有的特征，准确率不高。

> ![图像简单数据化无法保留图像特征](.\卷积神经网络\图像简单数据化无法保留图像特征.png)
>
> 视觉的角度，图像的本质内容并没有发生很大的变化，只是位置发生了变化。
>
> CNN用类似视觉的方式保留图像的特征，当图像翻转、旋转或者变换位置时，能有效的识别出是类似的图像。

## 人的视觉原理

![人的视觉原理](.\卷积神经网络\人的视觉原理.png)

最底层的特征基本是类似的，各种像素点组成的边缘，越往上，越能提取出物体类似的一些特征，到最上层，不同的高级特征最终组合成相应的图像，从而能够准确的区分不同的物体。

神经网络模拟人脑的这个特点，通过构造多层的神经网络，较低层的识别初级的图像特征，若干底层特征组成更上一层的特征，最终通过多个层级的组合，最终在顶层做出分类。

## CNN的基本原理

典型的CNN包括3个部分：卷积层、池化层、全连接层。

卷积层：提取图像的局部特征

池化层：大幅降低参数量级（降维）

全连接层：输出结果

cs231n课程中一个CNN各个层级结构图：

![各个层级](.\卷积神经网络\各个层级.jpg)

### 卷积计算层

CNN解决的一个问题保留原始特征，使得计算机能将那些仅仅只是做了平移、缩放、旋转、微变形等简单变换的图像识别出来。

![特征](.\卷积神经网络\特征.jpg)

CNN是一块一块的进行对比，这些小块成为Features，在两幅图中大致相同的位置找到一些粗糙的特征进行匹配，CNN能够更好的看到两幅图的相似性。

每一个feature就是一个小图（二维数组），不同的feature匹配图像中不同的特征。图中字母X中由对角线和交叉线组成的feature基本能够识别出大多数”X“所具有的重要特征。

![特征提取](.\卷积神经网络\特征提取.gif)

对图像（不同的数据窗口数据）和滤波矩阵（也称为**卷积核**）（一组固定的权重：因为每个神经元的多个权重固定，可以看成一个恒定的滤波器）做内积（逐个元素相乘再求和）的操作称为**卷积**。

![20160702215705128](.\卷积神经网络\20160702215705128.jpg)

左边是原始输入数据，中间是滤波器filter，右边是输出的新的二维数据。

在CNN中，filter（带一组固定权重的神经元）对局部输入数据进行卷积计算，每计算完一个数据窗口内的局部数据后，数据窗口不断平滑移动，直到计算完所有数据，过程中有如下几个参数：

①深度depth：神经元个数，决定输出的depth厚度，也代表滤波器个数。

②步长stride：决定滑动多少步到边缘。

③填充值zero-padding：在外围边缘补充若干圈0，方便从初始位置以步长为单位可以刚好滑到末尾位置，即为了总长能被步长整除。

![滤波器](.\卷积神经网络\滤波器.jpg)

卷积动态可视化图：

![可视化过程](.\卷积神经网络\可视化过程.gif)

- 左边7×7×3代表图像像素的长宽 ，3是RGB三个颜色通道
- 中间部分是两个不同的滤波器
- 两个神经元，depth=2，有两个滤波器
- 数据窗口每次移动2个步长取3×3的局部数据，stride=2
- zero-padding=1

分别以两个滤波器filter为轴滑动数组进行卷积计算，得到两组不同的结果。

> 注意到：左边数据在变化，每次滤波器都是针对某一局部的数据窗口进行卷积，即CNN中的**局部感知**机制。
>
> 与此同时，数据窗口滑动，导致输入在变化，但中间的滤波器filter W0的权重（即每个神经元连接数据窗口的权重）是固定不变的，这个权重不变即CNN的**参数（权重）共享**机制。

### 卷积层的反向传播

![反向传播](.\卷积神经网络\反向传播.jpg)

任务是计算$dW^{[l]}$，$db^{[l]}$（它们是与当前层参数相关的导数），以及$dA^{[l-1]}$（它将被传递给前一层）。

$dA^{[l-1]}$是输入，张量$dW$和$W$，$db,b$以及$dA,A$维度相同，第一步是求激活函数关于输入张量的导数，记作$dZ^{[l]}$，即：

$d Z^{[l]} = dA^{[l]}×g'(Z^{[l]})$

其中，$dA^{[l]} = \frac{\partial{L}}{\partial A^{[l]}}$，$L$是Loss function，其他参数类推。

$d A += \sum\limits_{m=0}^{n_h} \sum\limits_{n=0}^{n_w}W·dZ[m,n]$，其中$W$表示卷积核kernel，$dZ[m,n]$是一个标量，属于从前一层得到的偏导数。

### 池化层

可以看成是下采样，用于大大降低数据的维度，过程如下：

![池化层](.\卷积神经网络\池化层.gif)

原始图片是20×20的，对其进行下采样，采样窗口是10×10，最终将其下采样成为一个2×2的特征图。这样大大减少运算量，还可以有效的避免过拟合。

常用的有最大池化（MaxPooling）（取区域最大值）和平均池化（AveragePooling）（取区域平均值）。

使用max pooling，舍弃其他值，是否会造成信息损失问题：

> CNN中每一个卷积核可以看做一个特征提取器，不同的卷积核负责提取不同的特征，对其进行Max Pooling操作后，提取出的是真正能够识别特征的数值，其余被舍弃的数值，对于所要提取特定的特征并没有特别大的帮助。那么在进行后续计算使，减小了feature map的尺寸，从而减少参数，达到减小计算量。
>
> 并不是所有情况Max Pooling的效果都很好，有时候有些周边信息也会对某个特定特征的识别产生一定效果，那么这个时候舍弃这部分“不重要”的信息，就不划算了。具体情况具体分析，可以进行一个加与不加的结果输出对比，看最大池化是否有反效果。

### 全连接层

通过不断的设计卷积核的尺寸，数量，提取更多的特征，最后识别不同类别的物体。做完Max Pooling后，我们就会把这些数据“拍平”，丢到Flatten层，然后把Flatten层的output放到full connected Layer里，采用softmax对其进行分类。

![全连接层](.\卷积神经网络\全连接层.png)



## CNN应用

CNN擅长处理图像，如图像分类与检索、目标定位监测、目标分割（像素级的分类）

但是CNN缺乏足够的理论支撑，无法系统、完备的对其表现进行分析。

### CNN的一些经典案例

- **LeNet** ，第一个成功的卷积神经网络应用，是Yann LeCun在上世纪90年代实现的。当然，最著名还是被应用在识别数字和邮政编码等的LeNet结构。

- **AlexNet** ，AlexNet卷积神经网络在计算机视觉领域中受到欢迎，它由Alex Krizhevsky，Ilya Sutskever和Geoff Hinton实现。AlexNet在2012年的ImageNet ILSVRC 竞赛中夺冠，性能远远超出第二名（16%的top5错误率，第二名是26%的top5错误率）。这个网络的结构和LeNet非常类似，但是更深更大，并且使用了层叠的卷积层来获取特征（之前通常是只用一个卷积层并且在其后马上跟着一个汇聚层）。

- **ZF Net** ，Matthew Zeiler和Rob Fergus发明的网络在ILSVRC 2013比赛中夺冠，它被称为 ZFNet（Zeiler & Fergus Net的简称）。它通过修改结构中的超参数来实现对AlexNet的改良，具体说来就是增加了中间卷积层的尺寸，让第一层的步长和滤波器尺寸更小。

- **GoogLeNet** ，ILSVRC 2014的胜利者是谷歌的Szeged等实现的卷积神经网络。它主要的贡献就是实现了一个奠基模块，它能够显著地减少网络中参数的数量（AlexNet中有60M，该网络中只有4M）。还有，这个论文中没有使用卷积神经网络顶部使用全连接层，而是使用了一个平均汇聚，把大量不是很重要的参数都去除掉了。GooLeNet还有几种改进的版本，最新的一个是Inception-v4。

- **VGGNet** ，ILSVRC 2014的第二名是Karen Simonyan和 Andrew Zisserman实现的卷积神经网络，现在称其为VGGNet。它主要的贡献是展示出网络的深度是算法优良性能的关键部分。他们最好的网络包含了16个卷积/全连接层。网络的结构非常一致，从头到尾全部使用的是3x3的卷积和2x2的汇聚。他们的预训练模型是可以在网络上获得并在Caffe中使用的。VGGNet**不好的一点是它耗费更多计算资源，并且使用了更多的参数，导致更多的内存占用（140M）。其中绝大多数的参数都是来自于第一个全连接层。后来发现这些全连接层即使被去除，对于性能也没有什么影响**，这样就显著降低了参数数量。

- **ResNet** ，残差网络（Residual Network）是ILSVRC2015的胜利者，由何恺明等实现。它使用了特殊的跳跃链接，大量使用了批量归一化（batch normalization）。这个结构同样在最后没有使用全连接层。读者可以查看何恺明的的演讲（视频，PPT），以及一些使用Torch重现网络的实验。ResNet当前最好的卷积神经网络模型（2016年五月）。何开明等最近的工作是对原始结构做一些优化，*论文Identity Mappings in Deep Residual Networks，2016年3月发表*。

  

### CNN处理一维时间序列数据

卷积神经网络的应用范围越来越广，其中一个方向是时序信号特征提取，进行分类预测。

CNN 可以很好地识别出数据中的简单模式，然后使用这些简单模式在更高级的层中生成更复杂的模式。当你希望从整体数据集中较短的（固定长度）片段中获得感兴趣特征，并且该特性在该数据片段中的位置不具有高度相关性时，1D CNN 是非常有效的。

1D CNN 可以很好地应用于传感器数据的时间序列分析（比如陀螺仪或加速度计数据）；同样也可以很好地用于分析具有固定长度周期的信号数据（比如音频信号）。此外，它还能应用于自然语言处理的任务（由于单词的接近性可能并不总是一个可训练模式的好指标，因此 LSTM 网络在 NLP 中的应用更有前途）。

无论是一维、二维还是三维，卷积神经网络（CNNs）都具有相同的特点和相同的处理方法。关键区别在于输入数据的维数以及特征检测器（或滤波器）如何在数据之间滑动：

![1D CNN](.\卷积神经网络\1D_CNN.jpg)



## 参考资料

[卷积神经网络-CNN](https://easyai.tech/ai-definition/cnn/)

[卷积神经网络背后的数学原理](https://www.plob.org/article/18114.html)

[理解卷积神经网络-基于cs231n课程](https://blog.csdn.net/v_JULY_v/article/details/51812459)

[卷积神经网络基本原理详解](https://www.cnblogs.com/charlotte77/p/7759802.html)

[使用一维卷积神经网络处理时间序列数据](https://blog.csdn.net/xiaosongshine/article/details/88614450)

[卷积神经网络详解-谭庆波](https://zhuanlan.zhihu.com/p/47184529)