---
title: 迁移学习和深度迁移学习原理
date: 2020-5-18
tags:
- 迁移学习
- 深度迁移学习
categories:
- 笔记
mathjax: true
description: 主要分析迁移学习和深度迁移学习两篇综述文章
---

# 迁移学习

> S. J. Pan and Q. Yang, "**A Survey on Transfer Learning**" in *IEEE Transactions on Knowledge and Data Engineering*, vol. 22, no. 10, pp. 1345-1359
>
> ```pdf
> https://github.com/jiangxjun/document/blob/master/papers_notes/pdf_files/tkde_transfer_learning.pdf
> ```
>

## 迁移学习概念

传统机器学习的领域假设训练数据和测试数据属于相同的特征空间并在同一分布，然而现实中这种假设往往得不到满足。例如，我们对目标领域的分类问题，却只有源领域的训练数据，但源领域数据与目标领域数据要么不在同一个特征空间，要么不满足相同的数据分布，例如需要进行的文本分类语言是西班牙语。但只提供了了葡萄牙语的文本。

在某些情况下成功地进行知识迁移能够很大程度上提高学习的性能，但也同时降低了标记目标领域数据带来的大量时间和人力成本。

近年来，迁移学习已经成为一种解决**知识迁移问题**的新型学习框架，[这篇论文【S. J. Pan and Q. Yang, "A Survey on Transfer Learning," in *IEEE Transactions on Knowledge and Data Engineering*, vol. 22, no. 10, pp. 1345-1359】](https://ieeexplore.ieee.org/document/5288526)讨论了使用迁移学习进行分类、回归和聚类的一般过程，也讨论了迁移学习和其他相关的机器学习技术之间的关系，如领域适应性、多任务学习、样本选择和变量变换。

传统的机器学习/数据挖掘只有在训练集和测试集数据都来自同一个feature space和the same distribution的时候才有较好的表现，也就意味着如果每一次更换了数据集重新训练模型，会变得很麻烦。比如：

（1）从数据类型/内容上看，获取新的数据集很贵很麻烦

（2）从时间维度上看，有些数据很容易过期，不同时期的数据分布会不同。对于每个时间段都要进行一次训练很麻烦（如进行室内wifi定位）



2005年DARPA的信息处理办公室(IPTO)给迁移学习有了一个新的定义：一个系统具有把从先前的任务学习到的识别和应用知识的能力运用到新的任务上的能力。

区别于多任务学习(multi-task learning)中将source和target tasks同等对待，迁移学习更多的关注target tasks，在迁移学习中，两者不再对等。



## 几个概念

几个关键词：**domain（域）**和**task（任务）**，**source（源）**和**target（目标）**

domain包括两部分：feature space和probability（概率）。domain不同可能分为两种情况，特征空间不同或者概率不同。

task包括两部分：label space和objective predictive function（目标预测函数）。同样如上所述。

source：用于训练模型的域/任务。

target：使用基于source训练出来的模型对自己的数据进行预测/分类/聚类等机器学习任务的域/任务。



领域（domain）$\mathcal{D}$包括特征空间$\mathcal{X}$和边缘分布（marginal probability distribution）$P(X)$，$X = \{x_1,x_2,\dots,x_n\} \in \mathcal{X}$。即：$\mathcal{D}= \{\mathcal{X},P(X)\}$

任务（task）$\mathcal{T}$包括标签空间$\mathcal{Y}$和目标预测函数$f(·)$，也就是条件概率分布，记作$P(y|x)$。即：$\mathcal{T} = \{\mathcal{Y},f(·)\}$



**迁移学习定义**：给出一个源领域$\mathcal{T}_S$和学习任务$\mathcal{T}_S$，一个目标领域$\mathcal{D}_T$和学习任务$\mathcal{T}_T$，迁移学习指的是通过从$\mathcal{D}_S$和$\mathcal{T}_S$学到的知识了来帮助在$\mathcal{D}_T$中目标函数$f_T(·)$的学习。$\mathcal{D}_T \neq \mathcal{D}_S$或者$\mathcal{T}_T \neq \mathcal{T}_S$

源领域和目标领域不同指的是$\mathcal{X}_S \neq \mathcal{X}_T$【即特征空间不同】或者$P_S(X) \neq P_T(X)$【即源和目标任务的边缘分布不同】

源任务和目标任务不同指的是$\mathcal{Y}_S \neq \mathcal{Y}_T $【即任务标签集不同】或者$P(Y_S |X_S) \neq P(Y_T | X_T)$【即源和目标任务的条件概率不同】



## 迁移学习的划分

**①从问题角度来看**
（1）迁移什么？

哪一部分知识可以被迁移？

（2）怎么迁移？

那当然就是训练出适合的模型啦。

（3）什么时候需要用到迁移学习？

当source domain和target domain没什么关系或者太不相同的时候，迁移效果可能就不那么好了，甚至可能会比不迁移的时候表现要更差，这个就叫做**negative transfer**了。

可以看到，迁移学习的能力也是有限的，所以我们需要关注迁移学习的边界在哪里，比如用conditional Kolmogorov complexity去衡量tasks之间的相关性。



作者指出，现在很多工作都关注前两个问题，但实际上第三个问题是很重要的，因为你在那捣腾半天最后发现其实迁移了还不如不迁移，那不是白费心思嘛。所以作者认为这个问题应当被重视，比如说可以在迁移之前先看看source和domain之间的transferability（可迁移性）。



 **②从迁移场景来看**
（1）Homogeneous TL（同构学习）：source domain和target domain的feature space相同。

（2）Heterogeneous TL（异构学习）：source domain和target domain的feature space不同。



**③从迁移算法来看**
（1）Inductive TL（归纳式迁移学习）

source和target的domain可能一样或不一样，task不一样；target domain的labeled数据可得，source domain不一定可得。

所以呢，根据source domain的labeled数据可以再细分为两类：

multi_task learning（多任务学习）：source domain的labeled数据可得。s
self-taught learning（自学习）：source domain的labeled数据不可得。

![20170308151057479](/迁移学习/20170308151057479.png)

{% asset_img 20170308151057479.png 归纳式迁移学习 %}

> 代表性论文引用：Dai W, Yang Q,Xue G R, et al. Boosting for transfer learning[C]//Machine Learning, Proceedings of the Twenty-Fourth International   Conference. DBLP, 2007:193-200.



（2）Transductive TL（直推式迁移学习）

source和target的task一样，domain不一样；source domain的labeled数据可得，target domain的不可得。注意我们提过，domain不一样意味着两种可能：feature space不一样，或者feature space一样而probability不一样。而后一种情况和domain adaptation（域适配）息息相关。这里也可以根据domain和task的个数分为两个情况：

Domain Adaptation（域适配）：不同的domains+single task
Sample Selection Bias（样本选择偏差）/Covariance Shift（协方差转变）：single domain+single task

![20170308151131042](/迁移学习/20170308151131042.png)

{% asset_img 20170308151131042.png 直推式迁移学习 %}

> 代表性论文引用：Arnold A, Nallapati R, Cohen W W. A comparativestudy of methods for transductive transfer    learning[C]//Data Mining Workshops,2007. ICDM Workshops 2007. Seventh IEEE International Conference on. IEEE,2007: 77-82.



（3）Unsupervised TL（无监督迁移学习）

source和target的domain和task都不一样；source domain和target domain的labeled数据都不可得。

![20170308151150215](/迁移学习/20170308151150215.png)

{% asset_img 20170308151150215.png 无监督迁移学习 %}

> 代表性论文引用：Dai W, Yang Q, Xue G R, et al. Self-taughtclustering[C]//Proceedings of the 25th international conference on Machinelearning. ACM, 2008: 200-207.



将迁移学习根据领域和任务的不同进行了划分：

![分类](/迁移学习/分类.png)

{% asset_img 分类.png 迁移学习根据领域和任务不同分类 %}



综上，这几个方法差别主要是：**（1）source和domain之间，domain是否相同，task是否相同；（2）source domain和target domain的labeled数据是否可以得到。**

![tabel1](/迁移学习/tabel1.png)

{% asset_img tabel1.png Relation Between Traditional ML & Various Transfer Learning Setting %}

![table2](/迁移学习/table2.png)

{% asset_img table2.png Different Settings of Transfer Learning %}



**④从“迁移什么”来看**

（1）Instance-based TL（样本迁移）

尽管source domain数据不可以整个直接被用到target domain里，但是在source domain中还是找到一些可以重新被用到target domain中的数据。对它们调整权重，使它能与target domain中的数据匹配之后可以进行迁移。盗一张图，比如在这个例子中就是找到例子3，然后加重它的权值，这样在预测的时候它所占权重较大，预测也可以更准确。

instance reweighting（样本重新调整权重）和importance sampling（重要性采样）是instance-based TL里主要用到的两项技术。

![2018041120463545](/迁移学习/2018041120463545.png)

{% asset_img 2018041120463545.png 样本迁移 %}



（2）Feature-representation-transfer（特征迁移）

找到一些好的有代表性的特征，通过特征变换把source domain和target domain的特征变换到同样的空间，使得这个空间中source domain和target domain的数据具有相同的分布，然后进行传统的机器学习就可以了。

特征变换这一块可以举个栗子，比如评论男生的时候，你会说”好帅！好有男人味！好有担当！“；评论女生的时候，你会说”好漂亮！好有女人味！好温柔！“可以看出共同的特征就是“好看”。把“好帅”映射到“好看”，把“好漂亮”映射到“好看”，“好看”便是它们的共同特征。



（3）Parameter-transfer（参数/模型迁移）

假设source tasks和target tasks之间共享一些参数，或者共享模型hyperparameters（超参数）的先验分布。这样把原来的模型迁移到新的domain时，也可以达到不错的精度。

下面这个项目感觉用到就是这个parameter-transfer：[基于深度学习和迁移学习的识花实践](https://cosx.org/2017/10/transfer-learning/)。



（4）Relational-knowledge-transfer（关系迁移）

把相似的关系进行迁移，比如生物病毒传播到计算机病毒传播的迁移，比如师生关系到上司下属关系的迁移。

![table3](/迁移学习/table3.png)

{% asset_img table3.png Different Approaches Used in Different Settings %}



## 具体方法概述

即上表的扩展分析：

**①Inductive TL**

（1）Instances TL

主要方法：TrAdaBoost（AdaBoost的拓展）

假设：source domain和target domain数据的feature和labels是一样的，但是分布不一样；部分source domain的数据会对target domain的学习有帮助，但有部分可能会不利于target domain的学习。

过程：大致就是不断地给好的source data赋予更高的权重，给不好的赋予更多的权重。

（2）Features TL

需要根据source domain的labeled data是否可得分为两类（回顾：Inductive TL是target domain的labeled data可得，但是source domain的未必可得）：

Supervised Feature Construction（监督的特征构建）

Unsupervised Feature Construction（非监督的特征构建）

大致过程：通过减少model error，找出低维的有代表性的特征

（3）Parameters TL

主要方法：MT-IVM（基于Gaussian Processes）

大致过程（注意这里讲述的不是上面提到的那个主要方法的过程，而是另一个方法的过程）：假设source和target的参数都可以分为两部分，一部分是source/target特有的参数，一部分是它们共同有的参数。把这两个参数丢到改进了的SVM问题中，把参数训练出来就好了。

（4）Relational TL

注意和上面三种方法不同的是，这个方法是在relational domains里进行的，这个domain里的数据不是iid（独立同分布）的，所以它不需要假设每个domain里的数据都必须iid。

主要方法：statistical relational learning（SRL，统计关系学习）



**②Transductive TL**

Transductive TL是source domain的label可得，target domain的label不可得。但是要注意！为了得到target data的边际分布，在training的时候是需要一些unlabeled的target data的。

（1）Instances TL

主要方法：Importance sampling。

大致过程：我们的目标是最小化target domain里的expected risk（期望风险），但是target domain里没有labeled数据可用，所以我们必须替换成source domain里的数据，通过一些方法可以把它替换成source domain里的数据再乘以一个权重，只要把这个权重算出来就好。

（2）Feature Representations TL

主要方法：Structural Correspondence Learning（SCL）

大致过程：定义一些pivot features（就是共同特征），然后把每一个pivot feature都当成是一个新的label vector，通过公式把权重学习出来，然后对权重进行SVD分解，最后在argumented feature vector上使用传统的判别式算法即可。这里argumented feature vector包括这些新的features和所有原来的feature。（我还没有详细看这一块儿，所以先直译了）

难点：如何寻找好的pivot feature、domain之间的依赖性。



**③ Unsupervised TL**

（1）Feature Representations TL

主要方法：涉及两个

Self-taught clustering（STC），主要用于transfer clustering（迁移聚类）。目标就是希望通过source domain里大量的unlabeled data对少量的target domain里的unlabeled data进行聚类。
TDA方法，这个主要用于解决transfer dimensionality reduction（迁移降维）问题



## 目前存在的问题

（1）negative transfer的问题，比如怎么定义transferability，怎么衡量domain之间或task之间的相关性。

（2）目前的TL算法主要都是想要提高feature space相同probability不同时的表现的，domain不同或者task不同都有两种情况，一种是space不同，一种是space相同probability不同 ，但是这里说的就是**目前TL算法主要致力于提高的都是概率不同的**。但是很多时候我们其实也想要对feature space不同的domain和task进行迁移。即提到的heterogeneous TL（异构学习）问题。

（3）现在的TL主要都是应用到小且波动不大的数据集中（例如传感器数据、文本分类、图片分类等），以后要考虑如何用到更广泛的数据场景中。



## 迁移学习的基本过程

当可用的数据集特别少时，从头开始训练一个神经网络往往不能得到很好的结果，于是就从一个预训练模型开始训练，让网络本身已经具备一定的训练基础，然后用小数据集进行微调，便可以得到一个不错的结果。

通常加载预训练后，冻结模型的部分参数，一般只训练模型的最后几层，这样可以保留整个模型前面对物体特征提取的能力。预训练模型一定要与新的数据集有共同点，这样才能有效地预训练模型里的特征提取能力迁移到新的模型上。

一般过程：

1、加载预训练模型

2、冻结模型前面部分的参数

3、添加可训练的自定义的分类层，或使用原模型的分类层（如果可重用的话）

4、在新数据集上训练



## 准备数据集

按照50%，25%，25%的比例划分training，valid，和testing。

目录整理如下：

```python
/datadir
     /train
         /class1
         /class2
     
    /valid    验证集
         /class1
         /class2
    /test
         /class1
         /class2
```

预训练模型是基于ImageNet的，训练图像大小224×224，需要对数据集的图像进行大小缩放。



## 图像增广(data augmentation)

图像增广一般用来人工产生不同的图像，比如对图像进行旋转、翻转、随机裁剪、缩放等等。选择训练阶段对输入进行增广。



## 图像预处理

定义training和validation的预处理方式。

定义dataset和dataloader。用datasets.imagesfolder来定义dataset时，pytorch可以自动将图片与对应的文件夹分类对应起来，应用上面定义好的transformers，然后dataset传入到dataloader里，dataloader在每一个循环会自动生成batchsize大小的图像和label。

## ImageNet的预训练模型

PyTorch自带了很多ImageNet上的预训练模型

[PyTorch迁移学习实现图像分类](https://www.pytorchtutorial.com/pytorch-transfer-learning/)

[PyTorch 中文网](https://www.pytorchtutorial.com/)



# 深度迁移学习									

> 论文：A Survey on Deep Transfer Learning
>
> 清华大学智能技术与系统国家重点实验室近期发表的深度迁移学习综述，首次定义了深度迁移学习的四个分类，包括基于实例、映射、网络和对抗的迁移学习方法，并在每个方向上都给出了丰富的参考文献。
>
> [参考](https://zhuanlan.zhihu.com/p/44654536)



  <img src="/mypages/papers_notes/迁移学习/深度迁移学习论文.png" title="深度迁移学习论文" alt="深度学习论文"  />    

{% asset_img 深度迁移学习论文.png 深度迁移学习综述论文 %}

## 回顾迁移学习

**迁移学习**：解决训练数据不足这一基本问题，试图放松训练数据和测试数据必须是独立同分布(i.i.d)的假设，将知识从源域迁移到目标域。

![迁移学习形式化定义](/迁移学习/迁移学习形式化定义.PNG)

{% asset_img 迁移学习形式化定义.PNG 迁移学习形式化定义 %}

给出一个源领域$\mathcal{T}_S$和学习任务$\mathcal{T}_S$，一个目标领域$\mathcal{D}_T$和学习任务$\mathcal{T}_T$，迁移学习指的是通过从$\mathcal{D}_S$和$\mathcal{T}_S$学到的知识了来帮助在$\mathcal{D}_T$中目标函数$f_T(·)$的学习。$\mathcal{D}_T \neq \mathcal{D}_S$或者$\mathcal{T}_T \neq \mathcal{T}_S$

## 深度迁移学习

如何利用深度神经网络进行有效的知识传递很重要，即深度迁移学习。定义如下：

**深度迁移学习定义**：给定一个迁移学习任务，定义为$\left(\mathcal{D}_S,\mathcal{T}_S,\mathcal{D}_T,\mathcal{T}_T,f_{\mathcal{T}}(·)\right)$。其中$f_{\mathcal{T}}(·)$是一个反映深度神经网络的非线性函数。

**深度迁移学习分类**：基于实例的深度迁移学习、基于映射的深度迁移学习、基于网络的深度迁移学习和基于对抗的深度迁移学习。

### 基于实例的深度迁移学习

使用特定的权重调整策略，从源域中选择部分实例作为目标域训练集的补充，并为这些选择的实例分配合适的权值。

是基于这样的假设：“虽然两个域之间存在差异，但是源域中的部分实例可以被具有适当权重的目标域使用”。

### 基于映射的深度迁移学习

将实例从源域和目标域映射到新的数据空间，在这个新的数据空间中，来自两个域的实例是相似的，适合于联合深度神经网络。

是基于这样的假设：“尽管两个源域之间存在差异，但它们在一个复杂的新数据空间中可能更加相似”。

### 基于网络的深度迁移学习

将源领域中预先训练好的部分网络，包括其网络结构和连接参数，重新利用，将其转化为用于目标领域的深度神经网络的一部分。

是基于这样的假设：“神经网络类似于人脑的处理机制，是一个迭代的、连续的抽象过程”。该网络的前端层可以看作是一个特征提取器，所提取的特征是通用的。

### 基于对抗的深度迁移学习

在生成对抗网络(GAN)的启发下，引入对抗技术，寻找既适合于源域又适用于目标域的可迁移表达。

是基于这样的假设：“为了有效的迁移，良好的表征应该是对主要学习任务的区别性，以及对源域和目标域的不加区分”。

[更详细的中文描述](https://zhuanlan.zhihu.com/p/89951541)